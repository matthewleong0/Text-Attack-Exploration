{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Attack SKlearn Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the text attack and text attack recipes primarily against sklearn models: logistic regression and random forests. Two vectorizers are used: one for TF-IDF and the other Bag of Words. The attacks present in the notebook are the ones that could be plausibly used against SkLearn. 5 attacks were successfully implemented. The chosen problem set is reviews from rotten tomatoes. The classification is to label positive or negative sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import *\n",
    "from nltk import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, SimpleRNN, Conv1D, MaxPool1D, Bidirectional,\\\n",
    "Dropout, MaxPooling1D, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (C:/Users/mattl/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ca80b4e8fd43918f8482275f2c49e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using rotten tomatos IMBD dataset for this\n",
    "dataset = datasets.load_dataset('rotten_tomatoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  the rock is destined to be the 21st century's ...      1\n",
       "1  the gorgeously elaborate continuation of \" the...      1\n",
       "2                     effective but too-tepid biopic      1\n",
       "3  if you sometimes like to go to the movies to h...      1\n",
       "4  emerges as something rare , an issue movie tha...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for preprocessing\n",
    "def preprocess(row):\n",
    "\n",
    "    row = re.sub('[%\\n]', ' ', row)\n",
    "    row = re.sub('[%’\"\"-+\\,.:<>-]', '', row)\n",
    "    row = row.lstrip('[').rstrip(']')\n",
    "    row = re.sub('[^A-Za-z0-9 ]+','',row)\n",
    "\n",
    "    #lower case and remove white spaces\n",
    "    row = row.lower()\n",
    "    row = row.strip()\n",
    "\n",
    "    #tokenize and remove stopwords\n",
    "    tokens = tokenizer.tokenize(row)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    lemma = [wnl.lemmatize(w) for w in tokens]\n",
    "    return lemma\n",
    "\n",
    "def rejoin_words(row):\n",
    "    joined_words = ( \" \".join(row))\n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and remove stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# uncomment if need to download\n",
    "#nltk.download(\"stopwords\") \n",
    "#nltk.download('punkt')\n",
    "\n",
    "stop_words =set(stopwords.words(\"english\"))\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "for df in [train_df,test_df]:\n",
    "    df['words'] = df['text'].apply(preprocess)\n",
    "    df['processed'] = df['words'].apply(rejoin_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purposes of this exercise just focus on TFidf for the word embedding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#y = df['label'].values\n",
    "#X = df.drop(['real'],axis=1)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "x_train = train_df['processed'].values.tolist()\n",
    "x_test = test_df['processed'].values.tolist()\n",
    "tfidf_vectorizer = TfidfVectorizer().fit(x_train)\n",
    "x_train_matrix = tfidf_vectorizer.transform(x_train)\n",
    "x_test_matrix = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "words = tfidf_vectorizer.get_feature_names_out()\n",
    "x_train_tfidf = pd.DataFrame(x_train_matrix.todense(), columns = words)\n",
    "x_test_tfidf = pd.DataFrame(x_test_matrix.todense(), columns = words)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vect_fit = vectorizer.fit(df['processed'])\n",
    "    bag_words = vect_fit.transform(df['processed'])\n",
    "    df = pd.DataFrame(bag_words.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return df,vect_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.7479484173505275\n"
     ]
    }
   ],
   "source": [
    "#find the optimal regularization parameter using GridSearchCV\n",
    "params = {\"C\":[0.01,0.1,1,10]}\n",
    "lg = LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(lg, params)\n",
    "grid.fit(x_train_tfidf, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The roc_auc_score is: 0.8542287804174045\n",
      "The training accuracy is: 0.9881594372801876\n",
      "The test accuracy is : 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "# use the best parameters from gridserachCV\n",
    "lg_grid = LogisticRegression(penalty='l2',C=10)\n",
    "lg_grid.fit(x_train_tfidf,y_train)\n",
    "lr_pred_tr = lg_grid.predict(x_train_tfidf)\n",
    "lr_pred_te = lg_grid.predict(x_test_tfidf)\n",
    "print(\"The roc_auc_score is:\", roc_auc_score(y_test,lg_grid.predict_proba(x_test_tfidf)[:,1]))\n",
    "print(\"The training accuracy is:\",metrics.accuracy_score(y_train, lr_pred_tr))\n",
    "print(\"The test accuracy is :\", metrics.accuracy_score(y_test, lr_pred_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional bag of words model made\n",
    "train_bow_df,bow_fit = get_bow(train_df)\n",
    "# Bag of words cannot recognize words that are not in the training data\n",
    "# For the purposes of this hackathon, just focus on the training accuracy.\n",
    "#test_bow_df = get_bow(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [27:57<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n",
      "0.745369284876905\n"
     ]
    }
   ],
   "source": [
    "#find the optimal regularization parameter using GridSearchCV\n",
    "params = {\"C\":[0.01,0.1,1,10]}\n",
    "bow_lg = LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(bow_lg, params)\n",
    "grid.fit(train_bow_df, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: 0.9542790152403282\n"
     ]
    }
   ],
   "source": [
    "# use the best parameters from gridserachCV\n",
    "bow_lg_grid = LogisticRegression(penalty='l2',C=1)\n",
    "bow_lg_grid.fit(train_bow_df,y_train)\n",
    "lr_pred_tr = lg_grid.predict(train_bow_df)\n",
    "print(\"The training accuracy is:\",metrics.accuracy_score(y_train, lr_pred_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that takes a long time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'n_estimators': 300}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gridsearch bag of words\n",
    "params = {'n_estimators':[300,250,200,350],'max_depth':[None]}\n",
    "\n",
    "rf_optimal_params = GridSearchCV(RandomForestClassifier(random_state=10), params)\n",
    "rf_optimal_params.fit(x_train_tfidf, y_train)\n",
    "rf_optimal_params.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for random forest 0.724202626641651\n",
      "The roc_auc_score is: 0.7977130406316331\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74       533\n",
      "           1       0.76      0.66      0.70       533\n",
      "\n",
      "    accuracy                           0.72      1066\n",
      "   macro avg       0.73      0.72      0.72      1066\n",
      "weighted avg       0.73      0.72      0.72      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest TF-IDF version. \n",
    "random_forest = RandomForestClassifier(random_state = 10, \n",
    "                                       max_depth = None,\n",
    "                                       n_estimators = 300)\n",
    "random_forest.fit(x_train_tfidf,y_train)\n",
    "random_forest_tepred = random_forest.predict(x_test_tfidf)\n",
    "print(\"Testing accuracy for random forest\", metrics.accuracy_score(y_test,random_forest_tepred))\n",
    "print(\"The roc_auc_score is:\", roc_auc_score(y_test,random_forest.predict_proba(x_test_tfidf)[:,1]))\n",
    "print(classification_report(y_test, random_forest_tepred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to a file\n",
    "model_filename = \"random_forest_model.pkl\"\n",
    "joblib.dump(random_forest, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Random Forest Model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Random Forest model from the saved file\n",
    "rf_model = joblib.load(\"random_forest_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for random forest 0.724202626641651\n",
      "The roc_auc_score is: 0.7977130406316331\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74       533\n",
      "           1       0.76      0.66      0.70       533\n",
      "\n",
      "    accuracy                           0.72      1066\n",
      "   macro avg       0.73      0.72      0.72      1066\n",
      "weighted avg       0.73      0.72      0.72      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest_tepred = rf_model.predict(x_test_tfidf)\n",
    "print(\"Testing accuracy for random forest\", metrics.accuracy_score(y_test,random_forest_tepred))\n",
    "print(\"The roc_auc_score is:\", roc_auc_score(y_test,rf_model.predict_proba(x_test_tfidf)[:,1]))\n",
    "print(classification_report(y_test, random_forest_tepred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Attack Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for repeated code\n",
    "from textattack.models.wrappers import SklearnModelWrapper\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack import Attacker\n",
    "\n",
    "def do_attack(model_wrapper,attack_recipe,text_attack_data):\n",
    "    attack = attack_recipe.build(model_wrapper)\n",
    "    attacker = Attacker(attack,text_attack_data)\n",
    "    attacker.attack_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SkLearnModel wrapper used an outdated function so the code needs to be manually changed in the source package in order to get it to work."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextFoolerJin2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attack recipe is a word based subsistution attack. It does so by subsituting words with a similar meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_wrapper = SklearnModelWrapper(rf_model, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (C:/Users/mattl/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb7c10bab324341b49df42962ff0d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mtrain\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "text_attack_data = HuggingFaceDataset(\"rotten_tomatoes\", None, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.ensemble._forest.RandomForestClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.840845057\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:09<01:22,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|██▉                          | 1/10 [00:09<01:25,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Positive (74%)]] --> [[Negative (55%)]]\n",
      "\n",
      "the rock is [[destined]] to be the 21st century's [[new]] \" conan \" and that he's going to make a splash even [[greater]] than arnold schwarzenegger , jean-claud van damme or [[steven]] [[segal]] .\n",
      "\n",
      "the rock is [[intention]] to be the 21st century's [[newest]] \" conan \" and that he's going to make a splash even [[high]] than arnold schwarzenegger , jean-claud van damme or [[etienne]] [[adler]] .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|█████▊                       | 2/10 [00:10<00:42,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (72%)]] --> [[Negative (51%)]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[lord]] of the rings \" [[trilogy]] is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[vision]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[seigneur]] of the rings \" [[netherworld]] is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[brainchild]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:  30%|████████▋                    | 3/10 [00:10<00:25,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (79%)]]\n",
      "\n",
      "[[effective]] but too-tepid biopic\n",
      "\n",
      "[[helpful]] but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 0 / 0 / 4:  40%|███████████▌                 | 4/10 [00:11<00:17,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (75%)]] --> [[Negative (55%)]]\n",
      "\n",
      "if you sometimes like to go to the movies to have [[fun]] , wasabi is a good place to start .\n",
      "\n",
      "if you sometimes like to go to the movies to have [[amuse]] , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 0 / 0 / 5:  50%|██████████████▌              | 5/10 [00:11<00:11,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (64%)]] --> [[Negative (54%)]]\n",
      "\n",
      "emerges as something [[rare]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "emerges as something [[scarce]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 0 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:13<00:09,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (90%)]] --> [[Negative (51%)]]\n",
      "\n",
      "the film [[provides]] some great insight into the neurotic [[mindset]] of all comics -- even those who have [[reached]] the [[absolute]] [[top]] of the [[game]] .\n",
      "\n",
      "the film [[constitutes]] some great insight into the neurotic [[philosophy]] of all comics -- even those who have [[made]] the [[utter]] [[alto]] of the [[jeu]] .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 0 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:14<00:06,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[Negative (72%)]]\n",
      "\n",
      "offers that [[rare]] combination of entertainment and education .\n",
      "\n",
      "offers that [[scant]] combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 0 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:15<00:01,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (70%)]] --> [[Negative (51%)]]\n",
      "\n",
      "perhaps no [[picture]] ever made has more literally showed that the road to hell is paved with [[good]] intentions .\n",
      "\n",
      "perhaps no [[photo]] ever made has more literally showed that the road to hell is paved with [[adequate]] intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (59%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 9 / 0 / 1 / 10: 100%|███████████████████████████| 10/10 [00:16<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (87%)]] --> [[Negative (60%)]]\n",
      "\n",
      "take care of my cat offers a [[refreshingly]] different slice of asian [[cinema]] .\n",
      "\n",
      "take care of my cat offers a [[divinely]] different slice of asian [[movie]] .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 9      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 13.83% |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 81.11  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(rf_model_wrapper,TextFoolerJin2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when under attack the tfidf model's training accuracy goes down to an astonishing 0%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: TF-IDF and Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_model_wrapper = SklearnModelWrapper(lg_grid, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.840845057\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|██▉                          | 1/10 [00:06<01:01,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (64%)]]\n",
      "\n",
      "the rock is [[destined]] to be the 21st century's new \" conan \" and that he's going to [[make]] a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "the rock is [[meant]] to be the 21st century's new \" conan \" and that he's going to [[produce]] a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|█████▊                       | 2/10 [00:07<00:30,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[Negative (62%)]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[lord]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[vision]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[god]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[thinking]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:  30%|████████▋                    | 3/10 [00:08<00:18,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[Negative (51%)]]\n",
      "\n",
      "[[effective]] but too-tepid biopic\n",
      "\n",
      "[[helpful]] but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 0 / 0 / 4:  40%|███████████▌                 | 4/10 [00:08<00:12,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (68%)]] --> [[Negative (60%)]]\n",
      "\n",
      "if you sometimes like to go to the movies to have [[fun]] , wasabi is a good place to start .\n",
      "\n",
      "if you sometimes like to go to the movies to have [[joys]] , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 0 / 0 / 5:  50%|██████████████▌              | 5/10 [00:09<00:09,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[Negative (60%)]]\n",
      "\n",
      "emerges as something [[rare]] , an issue movie that's so [[honest]] and keenly observed that it doesn't feel like one .\n",
      "\n",
      "emerges as something [[scant]] , an issue movie that's so [[honesty]] and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 0 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:09<00:06,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (55%)]]\n",
      "\n",
      "the film [[provides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "the film [[gives]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 0 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:10<00:04,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (95%)]] --> [[Negative (86%)]]\n",
      "\n",
      "offers that [[rare]] combination of entertainment and [[education]] .\n",
      "\n",
      "offers that [[scant]] combination of entertainment and [[school]] .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 0 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:11<00:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[Negative (61%)]]\n",
      "\n",
      "perhaps no picture ever made has more literally [[showed]] that the [[road]] to hell is paved with good intentions .\n",
      "\n",
      "perhaps no picture ever made has more literally [[shown]] that the [[track]] to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 9 / 0 / 1 / 10: 100%|███████████████████████████| 10/10 [00:12<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (99%)]] --> [[Negative (74%)]]\n",
      "\n",
      "take care of my cat offers a [[refreshingly]] [[different]] slice of asian [[cinema]] .\n",
      "\n",
      "take care of my cat offers a [[splendidly]] [[disjointed]] slice of asian [[theater]] .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 9      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 12.9%  |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 75.56  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(lg_model_wrapper,TextFoolerJin2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the attack words well on the logistic regression model. This model's accuracy also goes down to 0%. For the sake of completeness, let us explore a bag of words logistic regression model. RF is excluded due to lower accuracy and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_lg_model_wrapper = SklearnModelWrapper(bow_lg_grid, bow_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.840845057\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|██▉                          | 1/10 [00:06<01:00,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (84%)]] --> [[Negative (73%)]]\n",
      "\n",
      "the rock is [[destined]] to be the 21st century's new \" conan \" and that he's going to [[make]] a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "the rock is [[meant]] to be the 21st century's new \" conan \" and that he's going to [[produce]] a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|█████▊                       | 2/10 [00:07<00:29,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (82%)]] --> [[Negative (51%)]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[lord]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[vision]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[seigneur]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[idea]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 1 / 0 / 3:  30%|████████▋                    | 3/10 [00:07<00:18,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[[FAILED]]]\n",
      "\n",
      "effective but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 1 / 0 / 4:  40%|███████████▌                 | 4/10 [00:08<00:12,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (57%)]] --> [[Negative (54%)]]\n",
      "\n",
      "if you sometimes like to go to the movies to have [[fun]] , wasabi is a good place to start .\n",
      "\n",
      "if you sometimes like to go to the movies to have [[amuse]] , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 1 / 0 / 5:  50%|██████████████▌              | 5/10 [00:08<00:08,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (54%)]]\n",
      "\n",
      "emerges as something [[rare]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "emerges as something [[scant]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 1 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:09<00:06,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[Negative (55%)]]\n",
      "\n",
      "the film [[provides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "the film [[gives]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 1 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:09<00:04,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[Negative (75%)]]\n",
      "\n",
      "offers that [[rare]] combination of entertainment and [[education]] .\n",
      "\n",
      "offers that [[sometimes]] combination of entertainment and [[school]] .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 1 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:10<00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[Negative (50%)]]\n",
      "\n",
      "perhaps no picture ever made has more literally showed that the [[road]] to [[hell]] is paved with good intentions .\n",
      "\n",
      "perhaps no picture ever made has more literally showed that the [[track]] to [[hellfire]] is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 1 / 1 / 10: 100%|███████████████████████████| 10/10 [00:11<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (98%)]] --> [[Negative (73%)]]\n",
      "\n",
      "take care of my cat offers a [[refreshingly]] [[different]] slice of asian [[cinema]] .\n",
      "\n",
      "take care of my cat offers a [[splendidly]] [[disjointed]] slice of asian [[theater]] .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 8      |\n",
      "| Number of failed attacks:     | 1      |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 10.0%  |\n",
      "| Attack success rate:          | 88.89% |\n",
      "| Average perturbed word %:     | 10.73% |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 67.67  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(bow_lg_model_wrapper,TextFoolerJin2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the bag of words algorithim is more robust in this case but that is due to the TextFoolerJin2019 algorithm being unable to swap out words more than the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist (checklist_ribeiro_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the documentation, this attack focuses on a number of attacks used in the Invariance Testing Method: Contraction, Extension, Changing Names, Number, Location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.attack_recipes import CheckList2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.ensemble._forest.RandomForestClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapExtend\n",
      "    (1): WordSwapContract\n",
      "    (2): WordSwapChangeName\n",
      "    (3): WordSwapChangeNumber\n",
      "    (4): WordSwapChangeLocation\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): RepeatModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-06 13:53:03,707 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|██▉                          | 1/10 [00:09<01:29,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (74%)]] --> [[[FAILED]]]\n",
      "\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|█████▊                       | 2/10 [00:13<00:53,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (72%)]] --> [[[FAILED]]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 3 / 0 / 3:  30%|████████▋                    | 3/10 [00:13<00:32,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[[FAILED]]]\n",
      "\n",
      "effective but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 4 / 0 / 4:  40%|███████████▌                 | 4/10 [00:14<00:21,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (75%)]] --> [[[FAILED]]]\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 5 / 0 / 5:  50%|██████████████▌              | 5/10 [00:17<00:17,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (64%)]] --> [[[FAILED]]]\n",
      "\n",
      "emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 6 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:19<00:12,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (90%)]] --> [[[FAILED]]]\n",
      "\n",
      "the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 7 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:19<00:08,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[[FAILED]]]\n",
      "\n",
      "offers that rare combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 8 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:20<00:02,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (70%)]] --> [[[FAILED]]]\n",
      "\n",
      "perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (59%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 9 / 1 / 10: 100%|███████████████████████████| 10/10 [00:20<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (87%)]] --> [[[FAILED]]]\n",
      "\n",
      "take care of my cat offers a refreshingly different slice of asian cinema .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+-------+\n",
      "| Attack Results                |       |\n",
      "+-------------------------------+-------+\n",
      "| Number of successful attacks: | 0     |\n",
      "| Number of failed attacks:     | 9     |\n",
      "| Number of skipped attacks:    | 1     |\n",
      "| Original accuracy:            | 90.0% |\n",
      "| Accuracy under attack:        | 90.0% |\n",
      "| Attack success rate:          | 0.0%  |\n",
      "| Average perturbed word %:     | nan%  |\n",
      "| Average num. words per input: | 19.5  |\n",
      "| Avg num queries:              | 6.56  |\n",
      "+-------------------------------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(rf_model_wrapper,CheckList2020,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: TF-IDF and Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapExtend\n",
      "    (1): WordSwapContract\n",
      "    (2): WordSwapChangeName\n",
      "    (3): WordSwapChangeNumber\n",
      "    (4): WordSwapChangeLocation\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): RepeatModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|██▉                          | 1/10 [00:04<00:43,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[[FAILED]]]\n",
      "\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|█████▊                       | 2/10 [00:08<00:33,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[[FAILED]]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 3 / 0 / 3:  30%|████████▋                    | 3/10 [00:08<00:20,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[[FAILED]]]\n",
      "\n",
      "effective but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 4 / 0 / 4:  40%|███████████▌                 | 4/10 [00:09<00:14,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (68%)]] --> [[[FAILED]]]\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 5 / 0 / 5:  50%|██████████████▌              | 5/10 [00:12<00:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[[FAILED]]]\n",
      "\n",
      "emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 6 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:13<00:09,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[[FAILED]]]\n",
      "\n",
      "the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 7 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:14<00:06,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (95%)]] --> [[[FAILED]]]\n",
      "\n",
      "offers that rare combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 8 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:15<00:01,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[[FAILED]]]\n",
      "\n",
      "perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 9 / 1 / 10: 100%|███████████████████████████| 10/10 [00:15<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (99%)]] --> [[[FAILED]]]\n",
      "\n",
      "take care of my cat offers a refreshingly different slice of asian cinema .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+-------+\n",
      "| Attack Results                |       |\n",
      "+-------------------------------+-------+\n",
      "| Number of successful attacks: | 0     |\n",
      "| Number of failed attacks:     | 9     |\n",
      "| Number of skipped attacks:    | 1     |\n",
      "| Original accuracy:            | 90.0% |\n",
      "| Accuracy under attack:        | 90.0% |\n",
      "| Attack success rate:          | 0.0%  |\n",
      "| Average perturbed word %:     | nan%  |\n",
      "| Average num. words per input: | 19.5  |\n",
      "| Avg num queries:              | 5.44  |\n",
      "+-------------------------------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(lg_model_wrapper,CheckList2020,text_attack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapExtend\n",
      "    (1): WordSwapContract\n",
      "    (2): WordSwapChangeName\n",
      "    (3): WordSwapChangeNumber\n",
      "    (4): WordSwapChangeLocation\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): RepeatModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|██▉                          | 1/10 [00:05<00:46,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (84%)]] --> [[[FAILED]]]\n",
      "\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|█████▊                       | 2/10 [00:08<00:35,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (82%)]] --> [[[FAILED]]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 3 / 0 / 3:  30%|████████▋                    | 3/10 [00:09<00:21,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[[FAILED]]]\n",
      "\n",
      "effective but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 4 / 0 / 4:  40%|███████████▌                 | 4/10 [00:09<00:14,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (57%)]] --> [[[FAILED]]]\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 5 / 0 / 5:  50%|██████████████▌              | 5/10 [00:12<00:12,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[[FAILED]]]\n",
      "\n",
      "emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 6 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:14<00:09,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[[FAILED]]]\n",
      "\n",
      "the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 7 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:14<00:06,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[[FAILED]]]\n",
      "\n",
      "offers that rare combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 8 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:15<00:01,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[[FAILED]]]\n",
      "\n",
      "perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 9 / 1 / 10: 100%|███████████████████████████| 10/10 [00:15<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (98%)]] --> [[[FAILED]]]\n",
      "\n",
      "take care of my cat offers a refreshingly different slice of asian cinema .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+-------+\n",
      "| Attack Results                |       |\n",
      "+-------------------------------+-------+\n",
      "| Number of successful attacks: | 0     |\n",
      "| Number of failed attacks:     | 9     |\n",
      "| Number of skipped attacks:    | 1     |\n",
      "| Original accuracy:            | 90.0% |\n",
      "| Accuracy under attack:        | 90.0% |\n",
      "| Attack success rate:          | 0.0%  |\n",
      "| Average perturbed word %:     | nan%  |\n",
      "| Average num. words per input: | 19.5  |\n",
      "| Avg num queries:              | 5.44  |\n",
      "+-------------------------------+-------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(bow_lg_model_wrapper,CheckList2020,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this attack method is unable to attack this model effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLARE (clare_li_2020) - Not Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithim constructs adversarial examples to test the robustness of NLP Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.attack_recipes import CLARE2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.ensemble._forest.RandomForestClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapMaskedLM(\n",
      "        (method):  bae\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.0005\n",
      "      )\n",
      "    (1): WordInsertionMaskedLM(\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.0\n",
      "      )\n",
      "    (2): WordMergeMaskedLM(\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.005\n",
      "      )\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): UniversalSentenceEncoder(\n",
      "        (metric):  cosine\n",
      "        (threshold):  0.7\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): RepeatModification\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'upos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf_model_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCLARE2020\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_attack_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mdo_attack\u001b[1;34m(model_wrapper, attack_recipe, text_attack_data)\u001b[0m\n\u001b[0;32m      8\u001b[0m attack \u001b[38;5;241m=\u001b[39m attack_recipe\u001b[38;5;241m.\u001b[39mbuild(model_wrapper)\n\u001b[0;32m      9\u001b[0m attacker \u001b[38;5;241m=\u001b[39m Attacker(attack,text_attack_data)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:441\u001b[0m, in \u001b[0;36mAttacker.attack_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attack_parallel()\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39msilent:\n\u001b[0;32m    444\u001b[0m     logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:170\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack\u001b[38;5;241m.\u001b[39mattack(example, ground_truth_output)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, SkippedAttackResult) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mattack_n\n\u001b[0;32m    173\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, SuccessfulAttackResult)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mnum_successful_examples\n\u001b[0;32m    176\u001b[0m ):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m worklist_candidates:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:168\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m     example\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabel_names\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:450\u001b[0m, in \u001b[0;36mAttack.attack\u001b[1;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal_function_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:398\u001b[0m, in \u001b[0;36mAttack._attack\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m    ``initial_result``.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     final_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cache()\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_result\u001b[38;5;241m.\u001b[39mgoal_status \u001b[38;5;241m==\u001b[39m GoalFunctionResultStatus\u001b[38;5;241m.\u001b[39mSUCCEEDED:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\search_method.py:36\u001b[0m, in \u001b[0;36mSearchMethod.__call__\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_transformations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch Method must have access to filter_transformations method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     )\n\u001b[1;32m---> 36\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[0;32m     38\u001b[0m result\u001b[38;5;241m.\u001b[39mnum_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_function\u001b[38;5;241m.\u001b[39mnum_queries\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\beam_search.py:32\u001b[0m, in \u001b[0;36mBeamSearch.perform_search\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m     30\u001b[0m potential_next_beam \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m beam:\n\u001b[1;32m---> 32\u001b[0m     transformations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     potential_next_beam \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transformations\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(potential_next_beam) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# If we did not find any possible perturbations, give up.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:305\u001b[0m, in \u001b[0;36mAttack.get_transformations\u001b[1;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation_cache[cache_key])\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_transformations_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mhashable(cache_key):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation_cache[cache_key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(transformed_texts)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:273\u001b[0m, in \u001b[0;36mAttack._get_transformations_uncached\u001b[1;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_transformations_uncached\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_text, original_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    264\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``self.transformation`` to ``text``, then filters the list\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    of possible transformations through the applicable constraints.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m        A filtered list of transformations where each transformation matches the constraints\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_transformation_constraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transformation_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transformed_texts\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\composite_transformation.py:39\u001b[0m, in \u001b[0;36mCompositeTransformation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m new_attacked_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations:\n\u001b[1;32m---> 39\u001b[0m     new_attacked_texts\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(new_attacked_texts)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\transformation.py:57\u001b[0m, in \u001b[0;36mTransformation.__call__\u001b[1;34m(self, current_text, pre_transformation_constraints, indices_to_modify, shifted_idxs, return_indices)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_indices:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices_to_modify\n\u001b[1;32m---> 57\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[0;32m     59\u001b[0m     text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_transformation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\word_merges\\word_merge_masked_lm.py:162\u001b[0m, in \u001b[0;36mWordMergeMaskedLM._get_transformations\u001b[1;34m(self, current_text, indices_to_modify)\u001b[0m\n\u001b[0;32m    160\u001b[0m indices_to_modify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indices_to_modify)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# find indices that are suitable to merge\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m token_tags \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    163\u001b[0m     current_text\u001b[38;5;241m.\u001b[39mpos_of_word_index(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mnum_words)\n\u001b[0;32m    164\u001b[0m ]\n\u001b[0;32m    165\u001b[0m merge_indices \u001b[38;5;241m=\u001b[39m find_merge_index(token_tags)\n\u001b[0;32m    166\u001b[0m merged_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\word_merges\\word_merge_masked_lm.py:163\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    160\u001b[0m indices_to_modify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indices_to_modify)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# find indices that are suitable to merge\u001b[39;00m\n\u001b[0;32m    162\u001b[0m token_tags \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 163\u001b[0m     \u001b[43mcurrent_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_of_word_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mnum_words)\n\u001b[0;32m    164\u001b[0m ]\n\u001b[0;32m    165\u001b[0m merge_indices \u001b[38;5;241m=\u001b[39m find_merge_index(token_tags)\n\u001b[0;32m    166\u001b[0m merged_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\shared\\attacked_text.py:141\u001b[0m, in \u001b[0;36mAttackedText.pos_of_word_index\u001b[1;34m(self, desired_word_idx)\u001b[0m\n\u001b[0;32m    139\u001b[0m     textattack\u001b[38;5;241m.\u001b[39mshared\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mflair_tag(sentence)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos_tags \u001b[38;5;241m=\u001b[39m sentence\n\u001b[1;32m--> 141\u001b[0m flair_word_list, flair_pos_list \u001b[38;5;241m=\u001b[39m \u001b[43mtextattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzip_flair_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pos_tags\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords):\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    147\u001b[0m         word \u001b[38;5;129;01min\u001b[39;00m flair_word_list\n\u001b[0;32m    148\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword absent in flair returned part-of-speech tags\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\shared\\utils\\strings.py:245\u001b[0m, in \u001b[0;36mzip_flair_result\u001b[1;34m(pred, tag_type)\u001b[0m\n\u001b[0;32m    243\u001b[0m word_list\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tag_type:\n\u001b[1;32m--> 245\u001b[0m     pos_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotation_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_value)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tag_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    247\u001b[0m     pos_list\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mget_label(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'upos'"
     ]
    }
   ],
   "source": [
    "do_attack(rf_model_wrapper,CLARE2020,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: TF-IDF and Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapMaskedLM(\n",
      "        (method):  bae\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.0005\n",
      "      )\n",
      "    (1): WordInsertionMaskedLM(\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.0\n",
      "      )\n",
      "    (2): WordMergeMaskedLM(\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.005\n",
      "      )\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): UniversalSentenceEncoder(\n",
      "        (metric):  cosine\n",
      "        (threshold):  0.7\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): RepeatModification\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'upos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlg_model_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCLARE2020\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_attack_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mdo_attack\u001b[1;34m(model_wrapper, attack_recipe, text_attack_data)\u001b[0m\n\u001b[0;32m      8\u001b[0m attack \u001b[38;5;241m=\u001b[39m attack_recipe\u001b[38;5;241m.\u001b[39mbuild(model_wrapper)\n\u001b[0;32m      9\u001b[0m attacker \u001b[38;5;241m=\u001b[39m Attacker(attack,text_attack_data)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:441\u001b[0m, in \u001b[0;36mAttacker.attack_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attack_parallel()\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39msilent:\n\u001b[0;32m    444\u001b[0m     logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:170\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack\u001b[38;5;241m.\u001b[39mattack(example, ground_truth_output)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, SkippedAttackResult) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mattack_n\n\u001b[0;32m    173\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, SuccessfulAttackResult)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mnum_successful_examples\n\u001b[0;32m    176\u001b[0m ):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m worklist_candidates:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:168\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m     example\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabel_names\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:450\u001b[0m, in \u001b[0;36mAttack.attack\u001b[1;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal_function_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:398\u001b[0m, in \u001b[0;36mAttack._attack\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m    ``initial_result``.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     final_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cache()\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_result\u001b[38;5;241m.\u001b[39mgoal_status \u001b[38;5;241m==\u001b[39m GoalFunctionResultStatus\u001b[38;5;241m.\u001b[39mSUCCEEDED:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\search_method.py:36\u001b[0m, in \u001b[0;36mSearchMethod.__call__\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_transformations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch Method must have access to filter_transformations method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     )\n\u001b[1;32m---> 36\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[0;32m     38\u001b[0m result\u001b[38;5;241m.\u001b[39mnum_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_function\u001b[38;5;241m.\u001b[39mnum_queries\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\beam_search.py:32\u001b[0m, in \u001b[0;36mBeamSearch.perform_search\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m     30\u001b[0m potential_next_beam \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m beam:\n\u001b[1;32m---> 32\u001b[0m     transformations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     potential_next_beam \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transformations\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(potential_next_beam) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# If we did not find any possible perturbations, give up.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:305\u001b[0m, in \u001b[0;36mAttack.get_transformations\u001b[1;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation_cache[cache_key])\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_transformations_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mhashable(cache_key):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation_cache[cache_key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(transformed_texts)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:273\u001b[0m, in \u001b[0;36mAttack._get_transformations_uncached\u001b[1;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_transformations_uncached\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_text, original_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    264\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``self.transformation`` to ``text``, then filters the list\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    of possible transformations through the applicable constraints.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m        A filtered list of transformations where each transformation matches the constraints\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_transformation_constraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transformation_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transformed_texts\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\composite_transformation.py:39\u001b[0m, in \u001b[0;36mCompositeTransformation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m new_attacked_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations:\n\u001b[1;32m---> 39\u001b[0m     new_attacked_texts\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(new_attacked_texts)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\transformation.py:57\u001b[0m, in \u001b[0;36mTransformation.__call__\u001b[1;34m(self, current_text, pre_transformation_constraints, indices_to_modify, shifted_idxs, return_indices)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_indices:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices_to_modify\n\u001b[1;32m---> 57\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[0;32m     59\u001b[0m     text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_transformation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\word_merges\\word_merge_masked_lm.py:162\u001b[0m, in \u001b[0;36mWordMergeMaskedLM._get_transformations\u001b[1;34m(self, current_text, indices_to_modify)\u001b[0m\n\u001b[0;32m    160\u001b[0m indices_to_modify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indices_to_modify)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# find indices that are suitable to merge\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m token_tags \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    163\u001b[0m     current_text\u001b[38;5;241m.\u001b[39mpos_of_word_index(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mnum_words)\n\u001b[0;32m    164\u001b[0m ]\n\u001b[0;32m    165\u001b[0m merge_indices \u001b[38;5;241m=\u001b[39m find_merge_index(token_tags)\n\u001b[0;32m    166\u001b[0m merged_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\word_merges\\word_merge_masked_lm.py:163\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    160\u001b[0m indices_to_modify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indices_to_modify)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# find indices that are suitable to merge\u001b[39;00m\n\u001b[0;32m    162\u001b[0m token_tags \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 163\u001b[0m     \u001b[43mcurrent_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_of_word_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mnum_words)\n\u001b[0;32m    164\u001b[0m ]\n\u001b[0;32m    165\u001b[0m merge_indices \u001b[38;5;241m=\u001b[39m find_merge_index(token_tags)\n\u001b[0;32m    166\u001b[0m merged_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\shared\\attacked_text.py:141\u001b[0m, in \u001b[0;36mAttackedText.pos_of_word_index\u001b[1;34m(self, desired_word_idx)\u001b[0m\n\u001b[0;32m    139\u001b[0m     textattack\u001b[38;5;241m.\u001b[39mshared\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mflair_tag(sentence)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos_tags \u001b[38;5;241m=\u001b[39m sentence\n\u001b[1;32m--> 141\u001b[0m flair_word_list, flair_pos_list \u001b[38;5;241m=\u001b[39m \u001b[43mtextattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzip_flair_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pos_tags\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords):\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    147\u001b[0m         word \u001b[38;5;129;01min\u001b[39;00m flair_word_list\n\u001b[0;32m    148\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword absent in flair returned part-of-speech tags\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\shared\\utils\\strings.py:245\u001b[0m, in \u001b[0;36mzip_flair_result\u001b[1;34m(pred, tag_type)\u001b[0m\n\u001b[0;32m    243\u001b[0m word_list\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tag_type:\n\u001b[1;32m--> 245\u001b[0m     pos_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotation_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_value)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tag_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    247\u001b[0m     pos_list\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mget_label(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'upos'"
     ]
    }
   ],
   "source": [
    "do_attack(lg_model_wrapper,CLARE2020,text_attack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapMaskedLM(\n",
      "        (method):  bae\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.0005\n",
      "      )\n",
      "    (1): WordInsertionMaskedLM(\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.0\n",
      "      )\n",
      "    (2): WordMergeMaskedLM(\n",
      "        (masked_lm_name):  RobertaForCausalLM\n",
      "        (max_length):  512\n",
      "        (max_candidates):  50\n",
      "        (min_confidence):  0.005\n",
      "      )\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): UniversalSentenceEncoder(\n",
      "        (metric):  cosine\n",
      "        (threshold):  0.7\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): RepeatModification\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                           | 0/10 [01:57<?, ?it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'upos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbow_lg_model_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCLARE2020\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_attack_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mdo_attack\u001b[1;34m(model_wrapper, attack_recipe, text_attack_data)\u001b[0m\n\u001b[0;32m      8\u001b[0m attack \u001b[38;5;241m=\u001b[39m attack_recipe\u001b[38;5;241m.\u001b[39mbuild(model_wrapper)\n\u001b[0;32m      9\u001b[0m attacker \u001b[38;5;241m=\u001b[39m Attacker(attack,text_attack_data)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:441\u001b[0m, in \u001b[0;36mAttacker.attack_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attack_parallel()\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39msilent:\n\u001b[0;32m    444\u001b[0m     logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:170\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack\u001b[38;5;241m.\u001b[39mattack(example, ground_truth_output)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, SkippedAttackResult) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mattack_n\n\u001b[0;32m    173\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, SuccessfulAttackResult)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mnum_successful_examples\n\u001b[0;32m    176\u001b[0m ):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m worklist_candidates:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:168\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m     example\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabel_names\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:450\u001b[0m, in \u001b[0;36mAttack.attack\u001b[1;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal_function_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:398\u001b[0m, in \u001b[0;36mAttack._attack\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m    ``initial_result``.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     final_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cache()\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_result\u001b[38;5;241m.\u001b[39mgoal_status \u001b[38;5;241m==\u001b[39m GoalFunctionResultStatus\u001b[38;5;241m.\u001b[39mSUCCEEDED:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\search_method.py:36\u001b[0m, in \u001b[0;36mSearchMethod.__call__\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_transformations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch Method must have access to filter_transformations method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     )\n\u001b[1;32m---> 36\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[0;32m     38\u001b[0m result\u001b[38;5;241m.\u001b[39mnum_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_function\u001b[38;5;241m.\u001b[39mnum_queries\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\beam_search.py:32\u001b[0m, in \u001b[0;36mBeamSearch.perform_search\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m     30\u001b[0m potential_next_beam \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m beam:\n\u001b[1;32m---> 32\u001b[0m     transformations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     potential_next_beam \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transformations\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(potential_next_beam) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# If we did not find any possible perturbations, give up.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:305\u001b[0m, in \u001b[0;36mAttack.get_transformations\u001b[1;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation_cache[cache_key])\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_transformations_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mhashable(cache_key):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation_cache[cache_key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(transformed_texts)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:273\u001b[0m, in \u001b[0;36mAttack._get_transformations_uncached\u001b[1;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_transformations_uncached\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_text, original_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    264\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``self.transformation`` to ``text``, then filters the list\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    of possible transformations through the applicable constraints.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m        A filtered list of transformations where each transformation matches the constraints\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_transformation_constraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transformation_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transformed_texts\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\composite_transformation.py:39\u001b[0m, in \u001b[0;36mCompositeTransformation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m new_attacked_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations:\n\u001b[1;32m---> 39\u001b[0m     new_attacked_texts\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(new_attacked_texts)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\transformation.py:57\u001b[0m, in \u001b[0;36mTransformation.__call__\u001b[1;34m(self, current_text, pre_transformation_constraints, indices_to_modify, shifted_idxs, return_indices)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_indices:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices_to_modify\n\u001b[1;32m---> 57\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[0;32m     59\u001b[0m     text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_transformation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\word_merges\\word_merge_masked_lm.py:162\u001b[0m, in \u001b[0;36mWordMergeMaskedLM._get_transformations\u001b[1;34m(self, current_text, indices_to_modify)\u001b[0m\n\u001b[0;32m    160\u001b[0m indices_to_modify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indices_to_modify)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# find indices that are suitable to merge\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m token_tags \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    163\u001b[0m     current_text\u001b[38;5;241m.\u001b[39mpos_of_word_index(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mnum_words)\n\u001b[0;32m    164\u001b[0m ]\n\u001b[0;32m    165\u001b[0m merge_indices \u001b[38;5;241m=\u001b[39m find_merge_index(token_tags)\n\u001b[0;32m    166\u001b[0m merged_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\word_merges\\word_merge_masked_lm.py:163\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    160\u001b[0m indices_to_modify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indices_to_modify)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# find indices that are suitable to merge\u001b[39;00m\n\u001b[0;32m    162\u001b[0m token_tags \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 163\u001b[0m     \u001b[43mcurrent_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_of_word_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mnum_words)\n\u001b[0;32m    164\u001b[0m ]\n\u001b[0;32m    165\u001b[0m merge_indices \u001b[38;5;241m=\u001b[39m find_merge_index(token_tags)\n\u001b[0;32m    166\u001b[0m merged_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\shared\\attacked_text.py:141\u001b[0m, in \u001b[0;36mAttackedText.pos_of_word_index\u001b[1;34m(self, desired_word_idx)\u001b[0m\n\u001b[0;32m    139\u001b[0m     textattack\u001b[38;5;241m.\u001b[39mshared\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mflair_tag(sentence)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos_tags \u001b[38;5;241m=\u001b[39m sentence\n\u001b[1;32m--> 141\u001b[0m flair_word_list, flair_pos_list \u001b[38;5;241m=\u001b[39m \u001b[43mtextattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzip_flair_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pos_tags\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords):\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    147\u001b[0m         word \u001b[38;5;129;01min\u001b[39;00m flair_word_list\n\u001b[0;32m    148\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword absent in flair returned part-of-speech tags\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\shared\\utils\\strings.py:245\u001b[0m, in \u001b[0;36mzip_flair_result\u001b[1;34m(pred, tag_type)\u001b[0m\n\u001b[0;32m    243\u001b[0m word_list\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tag_type:\n\u001b[1;32m--> 245\u001b[0m     pos_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotation_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_value)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tag_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    247\u001b[0m     pos_list\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mget_label(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'upos'"
     ]
    }
   ],
   "source": [
    "do_attack(bow_lg_model_wrapper,CLARE2020,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attack appears to not work against sklearn models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Alzantot Genetic Algorithm (faster_genetic_algorithm_jia_2019) - Not Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Algorithm implements an optimization algorithm inspired by the proccess of natural selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.attack_recipes import FasterGeneticAlgorithmJia2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.ensemble._forest.RandomForestClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): AlzantotGeneticAlgorithm(\n",
      "    (pop_size):  60\n",
      "    (max_iters):  40\n",
      "    (temp):  0.3\n",
      "    (give_up_if_no_improvement):  False\n",
      "    (post_crossover_check):  False\n",
      "    (max_crossover_retries):  20\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  8\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.2\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (max_mse_dist):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): LearningToWriteLanguageModel(\n",
      "        (max_log_prob_diff):  5.0\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index is out of bounds for dimension with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:35\u001b[0m, in \u001b[0;36mQueryHandler.query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswapped_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:76\u001b[0m, in \u001b[0;36mQueryHandler.try_query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     73\u001b[0m all_raw_idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     74\u001b[0m     raw_idx_list, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m     75\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m word_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_raw_idxs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minit_hidden(\u001b[38;5;28mlen\u001b[39m(batch))\n",
      "\u001b[1;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf_model_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFasterGeneticAlgorithmJia2019\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_attack_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mdo_attack\u001b[1;34m(model_wrapper, attack_recipe, text_attack_data)\u001b[0m\n\u001b[0;32m      8\u001b[0m attack \u001b[38;5;241m=\u001b[39m attack_recipe\u001b[38;5;241m.\u001b[39mbuild(model_wrapper)\n\u001b[0;32m      9\u001b[0m attacker \u001b[38;5;241m=\u001b[39m Attacker(attack,text_attack_data)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:441\u001b[0m, in \u001b[0;36mAttacker.attack_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attack_parallel()\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39msilent:\n\u001b[0;32m    444\u001b[0m     logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:170\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack\u001b[38;5;241m.\u001b[39mattack(example, ground_truth_output)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, SkippedAttackResult) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mattack_n\n\u001b[0;32m    173\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, SuccessfulAttackResult)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mnum_successful_examples\n\u001b[0;32m    176\u001b[0m ):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m worklist_candidates:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:168\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m     example\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabel_names\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:450\u001b[0m, in \u001b[0;36mAttack.attack\u001b[1;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal_function_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:398\u001b[0m, in \u001b[0;36mAttack._attack\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m    ``initial_result``.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     final_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cache()\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_result\u001b[38;5;241m.\u001b[39mgoal_status \u001b[38;5;241m==\u001b[39m GoalFunctionResultStatus\u001b[38;5;241m.\u001b[39mSUCCEEDED:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\search_method.py:36\u001b[0m, in \u001b[0;36mSearchMethod.__call__\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_transformations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch Method must have access to filter_transformations method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     )\n\u001b[1;32m---> 36\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[0;32m     38\u001b[0m result\u001b[38;5;241m.\u001b[39mnum_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_function\u001b[38;5;241m.\u001b[39mnum_queries\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\genetic_algorithm.py:236\u001b[0m, in \u001b[0;36mGeneticAlgorithm.perform_search\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     pop_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(population)\n\u001b[0;32m    238\u001b[0m     current_score \u001b[38;5;241m=\u001b[39m initial_result\u001b[38;5;241m.\u001b[39mscore\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\alzantot_genetic_algorithm.py:110\u001b[0m, in \u001b[0;36mAlzantotGeneticAlgorithm._initialize_population\u001b[1;34m(self, initial_result, pop_size)\u001b[0m\n\u001b[0;32m    108\u001b[0m words \u001b[38;5;241m=\u001b[39m initial_result\u001b[38;5;241m.\u001b[39mattacked_text\u001b[38;5;241m.\u001b[39mwords\n\u001b[0;32m    109\u001b[0m num_candidate_transformations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(words))\n\u001b[1;32m--> 110\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[0;32m    114\u001b[0m     diff_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28miter\u001b[39m(transformed_text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewly_modified_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    116\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:315\u001b[0m, in \u001b[0;36mAttack.get_transformations\u001b[1;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_transformations_uncached(\n\u001b[0;32m    312\u001b[0m         current_text, original_text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    313\u001b[0m     )\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:380\u001b[0m, in \u001b[0;36mAttack.filter_transformations\u001b[1;34m(self, transformed_texts, current_text, original_text)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints_cache[(current_text, transformed_text)]:\n\u001b[0;32m    379\u001b[0m             filtered_texts\u001b[38;5;241m.\u001b[39mappend(transformed_text)\n\u001b[1;32m--> 380\u001b[0m filtered_texts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_transformations_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43muncached_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_text\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# Sort transformations to ensure order is preserved between runs\u001b[39;00m\n\u001b[0;32m    384\u001b[0m filtered_texts\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:340\u001b[0m, in \u001b[0;36mAttack._filter_transformations_uncached\u001b[1;34m(self, transformed_texts, current_text, original_text)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m original_text:\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    337\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing `original_text` argument when constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(C)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is set to compare against `original_text`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m         )\n\u001b[1;32m--> 340\u001b[0m     filtered_texts \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     filtered_texts \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mcall_many(filtered_texts, current_text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\constraint.py:50\u001b[0m, in \u001b[0;36mConstraint.call_many\u001b[1;34m(self, transformed_texts, reference_text)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformed_text must have `last_transformation` attack_attr to apply constraint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         )\n\u001b[1;32m---> 50\u001b[0m filtered_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_constraint_many\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompatible_transformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_text\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(filtered_texts) \u001b[38;5;241m+\u001b[39m incompatible_transformed_texts\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\constraint.py:63\u001b[0m, in \u001b[0;36mConstraint._check_constraint_many\u001b[1;34m(self, transformed_texts, reference_text)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_constraint_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, transformed_texts, reference_text):\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Filters ``transformed_texts`` based on which transformations fulfill\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    the constraint. Calls ``check_constraint``\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        reference_texts (AttackedText): The ``AttackedText`` to compare against.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     64\u001b[0m         transformed_text\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_constraint(transformed_text, reference_text)\n\u001b[0;32m     67\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\constraint.py:66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_constraint_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, transformed_texts, reference_text):\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Filters ``transformed_texts`` based on which transformations fulfill\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    the constraint. Calls ``check_constraint``\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        reference_texts (AttackedText): The ``AttackedText`` to compare against.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     64\u001b[0m         transformed_text\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_constraint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\language_model_constraint.py:44\u001b[0m, in \u001b[0;36mLanguageModelConstraint._check_constraint\u001b[1;34m(self, transformed_text, reference_text)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot apply language model constraint without `newly_modified_indices`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[1;32m---> 44\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_log_probs_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(probs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: get_log_probs_at_index returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(probs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m values for 2 inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\learning_to_write.py:60\u001b[0m, in \u001b[0;36mLearningToWriteLanguageModel.get_log_probs_at_index\u001b[1;34m(self, text_list, word_index)\u001b[0m\n\u001b[0;32m     58\u001b[0m     queries\u001b[38;5;241m.\u001b[39mappend(query)\n\u001b[0;32m     59\u001b[0m     query_words\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[1;32m---> 60\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(log_probs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:40\u001b[0m, in \u001b[0;36mQueryHandler.query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sentences, swapped_words):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m         probs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     43\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING:  got runtime error trying languag emodel on language model w s/w\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m             s,\n\u001b[0;32m     45\u001b[0m             w,\n\u001b[0;32m     46\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:76\u001b[0m, in \u001b[0;36mQueryHandler.try_query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m num_idxs_dropped \u001b[38;5;241m=\u001b[39m orig_num_idxs \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_idx_list)\n\u001b[0;32m     73\u001b[0m all_raw_idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     74\u001b[0m     raw_idx_list, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m     75\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m word_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_raw_idxs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minit_hidden(\u001b[38;5;28mlen\u001b[39m(batch))\n\u001b[0;32m     78\u001b[0m source \u001b[38;5;241m=\u001b[39m word_idxs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "\u001b[1;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0"
     ]
    }
   ],
   "source": [
    "do_attack(rf_model_wrapper,FasterGeneticAlgorithmJia2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: TF-IDF and Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): AlzantotGeneticAlgorithm(\n",
      "    (pop_size):  60\n",
      "    (max_iters):  40\n",
      "    (temp):  0.3\n",
      "    (give_up_if_no_improvement):  False\n",
      "    (post_crossover_check):  False\n",
      "    (max_crossover_retries):  20\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  8\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.2\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (max_mse_dist):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): LearningToWriteLanguageModel(\n",
      "        (max_log_prob_diff):  5.0\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index is out of bounds for dimension with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:35\u001b[0m, in \u001b[0;36mQueryHandler.query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswapped_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:76\u001b[0m, in \u001b[0;36mQueryHandler.try_query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     73\u001b[0m all_raw_idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     74\u001b[0m     raw_idx_list, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m     75\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m word_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_raw_idxs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minit_hidden(\u001b[38;5;28mlen\u001b[39m(batch))\n",
      "\u001b[1;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlg_model_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFasterGeneticAlgorithmJia2019\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_attack_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mdo_attack\u001b[1;34m(model_wrapper, attack_recipe, text_attack_data)\u001b[0m\n\u001b[0;32m      8\u001b[0m attack \u001b[38;5;241m=\u001b[39m attack_recipe\u001b[38;5;241m.\u001b[39mbuild(model_wrapper)\n\u001b[0;32m      9\u001b[0m attacker \u001b[38;5;241m=\u001b[39m Attacker(attack,text_attack_data)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:441\u001b[0m, in \u001b[0;36mAttacker.attack_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attack_parallel()\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39msilent:\n\u001b[0;32m    444\u001b[0m     logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:170\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack\u001b[38;5;241m.\u001b[39mattack(example, ground_truth_output)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, SkippedAttackResult) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mattack_n\n\u001b[0;32m    173\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, SuccessfulAttackResult)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mnum_successful_examples\n\u001b[0;32m    176\u001b[0m ):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m worklist_candidates:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:168\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m     example\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabel_names\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:450\u001b[0m, in \u001b[0;36mAttack.attack\u001b[1;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal_function_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:398\u001b[0m, in \u001b[0;36mAttack._attack\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m    ``initial_result``.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     final_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cache()\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_result\u001b[38;5;241m.\u001b[39mgoal_status \u001b[38;5;241m==\u001b[39m GoalFunctionResultStatus\u001b[38;5;241m.\u001b[39mSUCCEEDED:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\search_method.py:36\u001b[0m, in \u001b[0;36mSearchMethod.__call__\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_transformations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch Method must have access to filter_transformations method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     )\n\u001b[1;32m---> 36\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[0;32m     38\u001b[0m result\u001b[38;5;241m.\u001b[39mnum_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_function\u001b[38;5;241m.\u001b[39mnum_queries\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\genetic_algorithm.py:236\u001b[0m, in \u001b[0;36mGeneticAlgorithm.perform_search\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     pop_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(population)\n\u001b[0;32m    238\u001b[0m     current_score \u001b[38;5;241m=\u001b[39m initial_result\u001b[38;5;241m.\u001b[39mscore\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\alzantot_genetic_algorithm.py:110\u001b[0m, in \u001b[0;36mAlzantotGeneticAlgorithm._initialize_population\u001b[1;34m(self, initial_result, pop_size)\u001b[0m\n\u001b[0;32m    108\u001b[0m words \u001b[38;5;241m=\u001b[39m initial_result\u001b[38;5;241m.\u001b[39mattacked_text\u001b[38;5;241m.\u001b[39mwords\n\u001b[0;32m    109\u001b[0m num_candidate_transformations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(words))\n\u001b[1;32m--> 110\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[0;32m    114\u001b[0m     diff_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28miter\u001b[39m(transformed_text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewly_modified_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    116\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:315\u001b[0m, in \u001b[0;36mAttack.get_transformations\u001b[1;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_transformations_uncached(\n\u001b[0;32m    312\u001b[0m         current_text, original_text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    313\u001b[0m     )\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:380\u001b[0m, in \u001b[0;36mAttack.filter_transformations\u001b[1;34m(self, transformed_texts, current_text, original_text)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints_cache[(current_text, transformed_text)]:\n\u001b[0;32m    379\u001b[0m             filtered_texts\u001b[38;5;241m.\u001b[39mappend(transformed_text)\n\u001b[1;32m--> 380\u001b[0m filtered_texts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_transformations_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43muncached_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_text\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# Sort transformations to ensure order is preserved between runs\u001b[39;00m\n\u001b[0;32m    384\u001b[0m filtered_texts\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:340\u001b[0m, in \u001b[0;36mAttack._filter_transformations_uncached\u001b[1;34m(self, transformed_texts, current_text, original_text)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m original_text:\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    337\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing `original_text` argument when constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(C)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is set to compare against `original_text`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m         )\n\u001b[1;32m--> 340\u001b[0m     filtered_texts \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     filtered_texts \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mcall_many(filtered_texts, current_text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\constraint.py:50\u001b[0m, in \u001b[0;36mConstraint.call_many\u001b[1;34m(self, transformed_texts, reference_text)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformed_text must have `last_transformation` attack_attr to apply constraint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         )\n\u001b[1;32m---> 50\u001b[0m filtered_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_constraint_many\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompatible_transformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_text\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(filtered_texts) \u001b[38;5;241m+\u001b[39m incompatible_transformed_texts\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\constraint.py:63\u001b[0m, in \u001b[0;36mConstraint._check_constraint_many\u001b[1;34m(self, transformed_texts, reference_text)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_constraint_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, transformed_texts, reference_text):\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Filters ``transformed_texts`` based on which transformations fulfill\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    the constraint. Calls ``check_constraint``\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        reference_texts (AttackedText): The ``AttackedText`` to compare against.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     64\u001b[0m         transformed_text\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_constraint(transformed_text, reference_text)\n\u001b[0;32m     67\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\constraint.py:66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_constraint_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, transformed_texts, reference_text):\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Filters ``transformed_texts`` based on which transformations fulfill\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    the constraint. Calls ``check_constraint``\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        reference_texts (AttackedText): The ``AttackedText`` to compare against.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     64\u001b[0m         transformed_text\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_constraint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\language_model_constraint.py:44\u001b[0m, in \u001b[0;36mLanguageModelConstraint._check_constraint\u001b[1;34m(self, transformed_text, reference_text)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot apply language model constraint without `newly_modified_indices`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[1;32m---> 44\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_log_probs_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(probs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: get_log_probs_at_index returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(probs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m values for 2 inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\learning_to_write.py:60\u001b[0m, in \u001b[0;36mLearningToWriteLanguageModel.get_log_probs_at_index\u001b[1;34m(self, text_list, word_index)\u001b[0m\n\u001b[0;32m     58\u001b[0m     queries\u001b[38;5;241m.\u001b[39mappend(query)\n\u001b[0;32m     59\u001b[0m     query_words\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[1;32m---> 60\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(log_probs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:40\u001b[0m, in \u001b[0;36mQueryHandler.query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sentences, swapped_words):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m         probs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     43\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING:  got runtime error trying languag emodel on language model w s/w\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m             s,\n\u001b[0;32m     45\u001b[0m             w,\n\u001b[0;32m     46\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:76\u001b[0m, in \u001b[0;36mQueryHandler.try_query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m num_idxs_dropped \u001b[38;5;241m=\u001b[39m orig_num_idxs \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_idx_list)\n\u001b[0;32m     73\u001b[0m all_raw_idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     74\u001b[0m     raw_idx_list, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m     75\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m word_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_raw_idxs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minit_hidden(\u001b[38;5;28mlen\u001b[39m(batch))\n\u001b[0;32m     78\u001b[0m source \u001b[38;5;241m=\u001b[39m word_idxs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "\u001b[1;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0"
     ]
    }
   ],
   "source": [
    "do_attack(lg_model_wrapper,FasterGeneticAlgorithmJia2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): AlzantotGeneticAlgorithm(\n",
      "    (pop_size):  60\n",
      "    (max_iters):  40\n",
      "    (temp):  0.3\n",
      "    (give_up_if_no_improvement):  False\n",
      "    (post_crossover_check):  False\n",
      "    (max_crossover_retries):  20\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  8\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.2\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (max_mse_dist):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): LearningToWriteLanguageModel(\n",
      "        (max_log_prob_diff):  5.0\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index is out of bounds for dimension with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:35\u001b[0m, in \u001b[0;36mQueryHandler.query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswapped_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:76\u001b[0m, in \u001b[0;36mQueryHandler.try_query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     73\u001b[0m all_raw_idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     74\u001b[0m     raw_idx_list, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m     75\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m word_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_raw_idxs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minit_hidden(\u001b[38;5;28mlen\u001b[39m(batch))\n",
      "\u001b[1;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbow_lg_model_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFasterGeneticAlgorithmJia2019\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_attack_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mdo_attack\u001b[1;34m(model_wrapper, attack_recipe, text_attack_data)\u001b[0m\n\u001b[0;32m      8\u001b[0m attack \u001b[38;5;241m=\u001b[39m attack_recipe\u001b[38;5;241m.\u001b[39mbuild(model_wrapper)\n\u001b[0;32m      9\u001b[0m attacker \u001b[38;5;241m=\u001b[39m Attacker(attack,text_attack_data)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:441\u001b[0m, in \u001b[0;36mAttacker.attack_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attack_parallel()\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39msilent:\n\u001b[0;32m    444\u001b[0m     logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:170\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack\u001b[38;5;241m.\u001b[39mattack(example, ground_truth_output)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, SkippedAttackResult) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mattack_n\n\u001b[0;32m    173\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, SuccessfulAttackResult)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mnum_successful_examples\n\u001b[0;32m    176\u001b[0m ):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m worklist_candidates:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attacker.py:168\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m     example\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabel_names\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:450\u001b[0m, in \u001b[0;36mAttack.attack\u001b[1;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal_function_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:398\u001b[0m, in \u001b[0;36mAttack._attack\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m    ``initial_result``.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     final_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cache()\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_result\u001b[38;5;241m.\u001b[39mgoal_status \u001b[38;5;241m==\u001b[39m GoalFunctionResultStatus\u001b[38;5;241m.\u001b[39mSUCCEEDED:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\search_method.py:36\u001b[0m, in \u001b[0;36mSearchMethod.__call__\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_transformations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch Method must have access to filter_transformations method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     )\n\u001b[1;32m---> 36\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[0;32m     38\u001b[0m result\u001b[38;5;241m.\u001b[39mnum_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_function\u001b[38;5;241m.\u001b[39mnum_queries\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\genetic_algorithm.py:236\u001b[0m, in \u001b[0;36mGeneticAlgorithm.perform_search\u001b[1;34m(self, initial_result)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     pop_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(population)\n\u001b[0;32m    238\u001b[0m     current_score \u001b[38;5;241m=\u001b[39m initial_result\u001b[38;5;241m.\u001b[39mscore\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\search_methods\\alzantot_genetic_algorithm.py:110\u001b[0m, in \u001b[0;36mAlzantotGeneticAlgorithm._initialize_population\u001b[1;34m(self, initial_result, pop_size)\u001b[0m\n\u001b[0;32m    108\u001b[0m words \u001b[38;5;241m=\u001b[39m initial_result\u001b[38;5;241m.\u001b[39mattacked_text\u001b[38;5;241m.\u001b[39mwords\n\u001b[0;32m    109\u001b[0m num_candidate_transformations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(words))\n\u001b[1;32m--> 110\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[0;32m    114\u001b[0m     diff_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28miter\u001b[39m(transformed_text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewly_modified_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    116\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:315\u001b[0m, in \u001b[0;36mAttack.get_transformations\u001b[1;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_transformations_uncached(\n\u001b[0;32m    312\u001b[0m         current_text, original_text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    313\u001b[0m     )\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:380\u001b[0m, in \u001b[0;36mAttack.filter_transformations\u001b[1;34m(self, transformed_texts, current_text, original_text)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints_cache[(current_text, transformed_text)]:\n\u001b[0;32m    379\u001b[0m             filtered_texts\u001b[38;5;241m.\u001b[39mappend(transformed_text)\n\u001b[1;32m--> 380\u001b[0m filtered_texts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_transformations_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43muncached_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_text\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# Sort transformations to ensure order is preserved between runs\u001b[39;00m\n\u001b[0;32m    384\u001b[0m filtered_texts\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack.py:340\u001b[0m, in \u001b[0;36mAttack._filter_transformations_uncached\u001b[1;34m(self, transformed_texts, current_text, original_text)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m original_text:\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    337\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing `original_text` argument when constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(C)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is set to compare against `original_text`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m         )\n\u001b[1;32m--> 340\u001b[0m     filtered_texts \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     filtered_texts \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mcall_many(filtered_texts, current_text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\constraint.py:50\u001b[0m, in \u001b[0;36mConstraint.call_many\u001b[1;34m(self, transformed_texts, reference_text)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformed_text must have `last_transformation` attack_attr to apply constraint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         )\n\u001b[1;32m---> 50\u001b[0m filtered_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_constraint_many\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompatible_transformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_text\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(filtered_texts) \u001b[38;5;241m+\u001b[39m incompatible_transformed_texts\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\constraint.py:63\u001b[0m, in \u001b[0;36mConstraint._check_constraint_many\u001b[1;34m(self, transformed_texts, reference_text)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_constraint_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, transformed_texts, reference_text):\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Filters ``transformed_texts`` based on which transformations fulfill\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    the constraint. Calls ``check_constraint``\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        reference_texts (AttackedText): The ``AttackedText`` to compare against.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     64\u001b[0m         transformed_text\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_constraint(transformed_text, reference_text)\n\u001b[0;32m     67\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\constraint.py:66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_constraint_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, transformed_texts, reference_text):\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Filters ``transformed_texts`` based on which transformations fulfill\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    the constraint. Calls ``check_constraint``\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        reference_texts (AttackedText): The ``AttackedText`` to compare against.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     64\u001b[0m         transformed_text\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_constraint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\language_model_constraint.py:44\u001b[0m, in \u001b[0;36mLanguageModelConstraint._check_constraint\u001b[1;34m(self, transformed_text, reference_text)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot apply language model constraint without `newly_modified_indices`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[1;32m---> 44\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_log_probs_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(probs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: get_log_probs_at_index returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(probs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m values for 2 inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\learning_to_write.py:60\u001b[0m, in \u001b[0;36mLearningToWriteLanguageModel.get_log_probs_at_index\u001b[1;34m(self, text_list, word_index)\u001b[0m\n\u001b[0;32m     58\u001b[0m     queries\u001b[38;5;241m.\u001b[39mappend(query)\n\u001b[0;32m     59\u001b[0m     query_words\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[1;32m---> 60\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(log_probs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:40\u001b[0m, in \u001b[0;36mQueryHandler.query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sentences, swapped_words):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m         probs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     43\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING:  got runtime error trying languag emodel on language model w s/w\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m             s,\n\u001b[0;32m     45\u001b[0m             w,\n\u001b[0;32m     46\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\constraints\\grammaticality\\language_models\\learning_to_write\\language_model_helpers.py:76\u001b[0m, in \u001b[0;36mQueryHandler.try_query\u001b[1;34m(self, sentences, swapped_words, batch_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m num_idxs_dropped \u001b[38;5;241m=\u001b[39m orig_num_idxs \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_idx_list)\n\u001b[0;32m     73\u001b[0m all_raw_idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     74\u001b[0m     raw_idx_list, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m     75\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m word_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_raw_idxs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minit_hidden(\u001b[38;5;28mlen\u001b[39m(batch))\n\u001b[0;32m     78\u001b[0m source \u001b[38;5;241m=\u001b[39m word_idxs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "\u001b[1;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0"
     ]
    }
   ],
   "source": [
    "do_attack(bow_lg_model_wrapper,FasterGeneticAlgorithmJia2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one does not work with SKLearn and as a result, the slower version most likely does not either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hotflip (hotflip_ebrahimi_2017) - Not Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of attack focuses on changing characters in the input text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.attack_recipes import HotFlipEbrahimi2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot perform GradientBasedWordSwap on model RandomForestClassifier(n_estimators=300, random_state=10).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf_model_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43mHotFlipEbrahimi2017\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_attack_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mdo_attack\u001b[1;34m(model_wrapper, attack_recipe, text_attack_data)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_attack\u001b[39m(model_wrapper,attack_recipe,text_attack_data):\n\u001b[1;32m----> 8\u001b[0m     attack \u001b[38;5;241m=\u001b[39m \u001b[43mattack_recipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     attacker \u001b[38;5;241m=\u001b[39m Attacker(attack,text_attack_data)\n\u001b[0;32m     10\u001b[0m     attacker\u001b[38;5;241m.\u001b[39mattack_dataset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack_recipes\\hotflip_ebrahimi_2017.py:40\u001b[0m, in \u001b[0;36mHotFlipEbrahimi2017.build\u001b[1;34m(model_wrapper)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(model_wrapper):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# \"HotFlip ... uses the gradient with respect to a one-hot input\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# representation to efficiently estimate which individual change has the\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# highest estimated loss.\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     transformation \u001b[38;5;241m=\u001b[39m \u001b[43mWordSwapGradientBased\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Don't modify the same word twice or stopwords\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     constraints \u001b[38;5;241m=\u001b[39m [RepeatModification(), StopwordModification()]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\word_swaps\\word_swap_gradient_based.py:43\u001b[0m, in \u001b[0;36mWordSwapGradientBased.__init__\u001b[1;34m(self, model_wrapper, top_n)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapper\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Make sure we know how to compute the gradient for this model.\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mvalidate_model_gradient_word_swap_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure this model has all of the required properties.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_input_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\shared\\validators.py:98\u001b[0m, in \u001b[0;36mvalidate_model_gradient_word_swap_compatibility\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot perform GradientBasedWordSwap on model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot perform GradientBasedWordSwap on model RandomForestClassifier(n_estimators=300, random_state=10)."
     ]
    }
   ],
   "source": [
    "do_attack(rf_model_wrapper,HotFlipEbrahimi2017,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: TF-IDF and Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot perform GradientBasedWordSwap on model LogisticRegression(C=10).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlg_model_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43mHotFlipEbrahimi2017\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_attack_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mdo_attack\u001b[1;34m(model_wrapper, attack_recipe, text_attack_data)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_attack\u001b[39m(model_wrapper,attack_recipe,text_attack_data):\n\u001b[1;32m----> 8\u001b[0m     attack \u001b[38;5;241m=\u001b[39m \u001b[43mattack_recipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     attacker \u001b[38;5;241m=\u001b[39m Attacker(attack,text_attack_data)\n\u001b[0;32m     10\u001b[0m     attacker\u001b[38;5;241m.\u001b[39mattack_dataset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack_recipes\\hotflip_ebrahimi_2017.py:40\u001b[0m, in \u001b[0;36mHotFlipEbrahimi2017.build\u001b[1;34m(model_wrapper)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(model_wrapper):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# \"HotFlip ... uses the gradient with respect to a one-hot input\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# representation to efficiently estimate which individual change has the\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# highest estimated loss.\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     transformation \u001b[38;5;241m=\u001b[39m \u001b[43mWordSwapGradientBased\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Don't modify the same word twice or stopwords\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     constraints \u001b[38;5;241m=\u001b[39m [RepeatModification(), StopwordModification()]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\word_swaps\\word_swap_gradient_based.py:43\u001b[0m, in \u001b[0;36mWordSwapGradientBased.__init__\u001b[1;34m(self, model_wrapper, top_n)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapper\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Make sure we know how to compute the gradient for this model.\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mvalidate_model_gradient_word_swap_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure this model has all of the required properties.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_input_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\shared\\validators.py:98\u001b[0m, in \u001b[0;36mvalidate_model_gradient_word_swap_compatibility\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot perform GradientBasedWordSwap on model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot perform GradientBasedWordSwap on model LogisticRegression(C=10)."
     ]
    }
   ],
   "source": [
    "do_attack(lg_model_wrapper,HotFlipEbrahimi2017,text_attack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot perform GradientBasedWordSwap on model LogisticRegression(C=1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbow_lg_model_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43mHotFlipEbrahimi2017\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext_attack_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mdo_attack\u001b[1;34m(model_wrapper, attack_recipe, text_attack_data)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_attack\u001b[39m(model_wrapper,attack_recipe,text_attack_data):\n\u001b[1;32m----> 8\u001b[0m     attack \u001b[38;5;241m=\u001b[39m \u001b[43mattack_recipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     attacker \u001b[38;5;241m=\u001b[39m Attacker(attack,text_attack_data)\n\u001b[0;32m     10\u001b[0m     attacker\u001b[38;5;241m.\u001b[39mattack_dataset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\attack_recipes\\hotflip_ebrahimi_2017.py:40\u001b[0m, in \u001b[0;36mHotFlipEbrahimi2017.build\u001b[1;34m(model_wrapper)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(model_wrapper):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# \"HotFlip ... uses the gradient with respect to a one-hot input\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# representation to efficiently estimate which individual change has the\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# highest estimated loss.\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     transformation \u001b[38;5;241m=\u001b[39m \u001b[43mWordSwapGradientBased\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Don't modify the same word twice or stopwords\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     constraints \u001b[38;5;241m=\u001b[39m [RepeatModification(), StopwordModification()]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\transformations\\word_swaps\\word_swap_gradient_based.py:43\u001b[0m, in \u001b[0;36mWordSwapGradientBased.__init__\u001b[1;34m(self, model_wrapper, top_n)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapper\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Make sure we know how to compute the gradient for this model.\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mvalidate_model_gradient_word_swap_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure this model has all of the required properties.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_input_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textattack\\shared\\validators.py:98\u001b[0m, in \u001b[0;36mvalidate_model_gradient_word_swap_compatibility\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot perform GradientBasedWordSwap on model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot perform GradientBasedWordSwap on model LogisticRegression(C=1)."
     ]
    }
   ],
   "source": [
    "do_attack(bow_lg_model_wrapper,HotFlipEbrahimi2017,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGA (iga_wang_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an improved genetic algorithim. It also poisons the input to cause training accuracy deficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.attack_recipes import IGAWang2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.ensemble._forest.RandomForestClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): ImprovedGeneticAlgorithm(\n",
      "    (pop_size):  60\n",
      "    (max_iters):  20\n",
      "    (temp):  0.3\n",
      "    (give_up_if_no_improvement):  False\n",
      "    (post_crossover_check):  False\n",
      "    (max_crossover_retries):  20\n",
      "    (max_replace_times_per_index):  5\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.2\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (max_mse_dist):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|██▉                          | 1/10 [00:17<02:34, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (74%)]] --> [[Negative (52%)]]\n",
      "\n",
      "the rock is [[destined]] to be the 21st century's new \" conan \" and that he's [[going]] to [[make]] a splash even greater than arnold schwarzenegger , jean-claud van damme or [[steven]] segal .\n",
      "\n",
      "the rock is [[designed]] to be the 21st century's new \" conan \" and that he's [[gonna]] to [[render]] a splash even greater than arnold schwarzenegger , jean-claud van damme or [[stevens]] segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|█████▊                       | 2/10 [00:27<01:49, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (72%)]] --> [[Negative (51%)]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[lord]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[vision]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[god]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[eyesight]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:  30%|████████▋                    | 3/10 [00:27<01:04,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (79%)]]\n",
      "\n",
      "[[effective]] but too-tepid biopic\n",
      "\n",
      "[[potency]] but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 0 / 0 / 4:  40%|███████████▌                 | 4/10 [00:29<00:44,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (75%)]] --> [[Negative (57%)]]\n",
      "\n",
      "if you sometimes like to go to the movies to have [[fun]] , wasabi is a good place to start .\n",
      "\n",
      "if you sometimes like to go to the movies to have [[pleasurable]] , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 0 / 0 / 5:  50%|██████████████▌              | 5/10 [00:31<00:31,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (64%)]] --> [[Negative (54%)]]\n",
      "\n",
      "emerges as something [[rare]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "emerges as something [[few]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 0 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:40<00:27,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (90%)]] --> [[Negative (50%)]]\n",
      "\n",
      "the film [[provides]] some [[great]] insight into the neurotic mindset of all comics -- even those who have reached the absolute [[top]] of the game .\n",
      "\n",
      "the film [[affords]] some [[noteworthy]] insight into the neurotic mindset of all comics -- even those who have reached the absolute [[superior]] of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 0 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:41<00:17,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[Negative (71%)]]\n",
      "\n",
      "offers that [[rare]] combination of entertainment and education .\n",
      "\n",
      "offers that [[rarity]] combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 0 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:43<00:04,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (70%)]] --> [[Negative (50%)]]\n",
      "\n",
      "perhaps no picture [[ever]] made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "perhaps no picture [[perpetually]] made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (59%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 9 / 0 / 1 / 10: 100%|███████████████████████████| 10/10 [00:53<00:00,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (87%)]] --> [[Negative (50%)]]\n",
      "\n",
      "take care of my cat offers a refreshingly [[different]] [[slice]] of asian [[cinema]] .\n",
      "\n",
      "take care of my cat offers a refreshingly [[numerous]] [[chopping]] of asian [[movie]] .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 9      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 12.01% |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 385.56 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(rf_model_wrapper,IGAWang2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: TF-IDF and Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): ImprovedGeneticAlgorithm(\n",
      "    (pop_size):  60\n",
      "    (max_iters):  20\n",
      "    (temp):  0.3\n",
      "    (give_up_if_no_improvement):  False\n",
      "    (post_crossover_check):  False\n",
      "    (max_crossover_retries):  20\n",
      "    (max_replace_times_per_index):  5\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.2\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (max_mse_dist):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|██▉                          | 1/10 [00:13<01:59, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (60%)]]\n",
      "\n",
      "the rock is [[destined]] to be the 21st century's new \" conan \" and that he's going to make a splash even [[greater]] than arnold schwarzenegger , jean-claud van damme or [[steven]] segal .\n",
      "\n",
      "the rock is [[meant]] to be the 21st century's new \" conan \" and that he's going to make a splash even [[stronger]] than arnold schwarzenegger , jean-claud van damme or [[stephen]] segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|█████▊                       | 2/10 [00:24<01:37, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[Negative (50%)]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot [[adequately]] describe co-writer/director peter jackson's expanded [[vision]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot [[sufficient]] describe co-writer/director peter jackson's expanded [[conception]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:  30%|████████▋                    | 3/10 [00:24<00:57,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[Negative (63%)]]\n",
      "\n",
      "[[effective]] but too-tepid biopic\n",
      "\n",
      "[[potency]] but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 0 / 0 / 4:  40%|███████████▌                 | 4/10 [00:26<00:39,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (68%)]] --> [[Negative (60%)]]\n",
      "\n",
      "if you sometimes like to go to the movies to have [[fun]] , wasabi is a good place to start .\n",
      "\n",
      "if you sometimes like to go to the movies to have [[amusing]] , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 0 / 0 / 5:  50%|██████████████▌              | 5/10 [00:30<00:30,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[Negative (81%)]]\n",
      "\n",
      "emerges as [[something]] [[rare]] , an issue movie that's so honest and [[keenly]] observed that it doesn't feel like one .\n",
      "\n",
      "emerges as [[nothing]] [[few]] , an issue movie that's so honest and [[passionately]] observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 0 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:34<00:22,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (57%)]]\n",
      "\n",
      "the film [[provides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "the film [[provide]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 0 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:36<00:15,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (95%)]] --> [[Negative (82%)]]\n",
      "\n",
      "offers that [[rare]] combination of entertainment and [[education]] .\n",
      "\n",
      "offers that [[few]] combination of entertainment and [[school]] .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 0 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:40<00:04,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[Negative (57%)]]\n",
      "\n",
      "perhaps no picture [[ever]] made has more literally [[showed]] that the road to hell is paved with [[good]] intentions .\n",
      "\n",
      "perhaps no picture [[increasingly]] made has more literally [[shown]] that the road to hell is paved with [[decent]] intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 9 / 0 / 1 / 10: 100%|███████████████████████████| 10/10 [00:55<00:00,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (99%)]] --> [[Negative (58%)]]\n",
      "\n",
      "take care of my [[cat]] offers a refreshingly [[different]] [[slice]] of asian [[cinema]] .\n",
      "\n",
      "take care of my [[cats]] offers a refreshingly [[various]] [[cutout]] of asian [[theater]] .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 9      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 15.3%  |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 557.78 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(lg_model_wrapper,IGAWang2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): ImprovedGeneticAlgorithm(\n",
      "    (pop_size):  60\n",
      "    (max_iters):  20\n",
      "    (temp):  0.3\n",
      "    (give_up_if_no_improvement):  False\n",
      "    (post_crossover_check):  False\n",
      "    (max_crossover_retries):  20\n",
      "    (max_replace_times_per_index):  5\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.2\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (max_mse_dist):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|██▉                          | 1/10 [00:14<02:07, 14.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (84%)]] --> [[Negative (58%)]]\n",
      "\n",
      "the rock is [[destined]] to be the 21st century's new \" conan \" and that he's going to [[make]] a splash even greater than arnold schwarzenegger , jean-claud van damme or [[steven]] segal .\n",
      "\n",
      "the rock is [[meant]] to be the 21st century's new \" conan \" and that he's going to [[render]] a splash even greater than arnold schwarzenegger , jean-claud van damme or [[stephen]] segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|█████▊                       | 2/10 [00:21<01:25, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (82%)]] --> [[Negative (51%)]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[lord]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[god]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:  30%|████████▋                    | 3/10 [00:21<00:50,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[Negative (54%)]]\n",
      "\n",
      "[[effective]] but too-tepid biopic\n",
      "\n",
      "[[potency]] but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 0 / 0 / 4:  40%|███████████▌                 | 4/10 [00:24<00:36,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (57%)]] --> [[Negative (71%)]]\n",
      "\n",
      "if you sometimes like to go to the movies to have [[fun]] , wasabi is a good place to start .\n",
      "\n",
      "if you sometimes like to go to the movies to have [[amusing]] , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 0 / 0 / 5:  50%|██████████████▌              | 5/10 [00:31<00:31,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (79%)]]\n",
      "\n",
      "[[emerges]] as something [[rare]] , an issue movie that's so honest and keenly [[observed]] that it doesn't feel like one .\n",
      "\n",
      "[[appear]] as something [[few]] , an issue movie that's so honest and keenly [[saw]] that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 0 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:35<00:23,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[Negative (59%)]]\n",
      "\n",
      "the film [[provides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "the film [[provide]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 0 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:36<00:15,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[Negative (50%)]]\n",
      "\n",
      "offers that [[rare]] [[combination]] of entertainment and education .\n",
      "\n",
      "offers that [[few]] [[mixing]] of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 0 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:40<00:04,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[Negative (60%)]]\n",
      "\n",
      "perhaps no picture ever made has more literally [[showed]] that the road to hell is paved with good [[intentions]] .\n",
      "\n",
      "perhaps no picture ever made has more literally [[shown]] that the road to hell is paved with good [[intention]] .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 9 / 0 / 1 / 10: 100%|███████████████████████████| 10/10 [00:55<00:00,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (98%)]] --> [[Negative (62%)]]\n",
      "\n",
      "[[take]] [[care]] of my [[cat]] offers a [[refreshingly]] different slice of asian [[cinema]] .\n",
      "\n",
      "[[taking]] [[caring]] of my [[cats]] offers a [[joyfully]] different slice of asian [[theater]] .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 9      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 15.24% |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 588.33 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(bow_lg_model_wrapper,IGAWang2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kuleshov2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an algorithim designed to generate adversial examples to poison the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.attack_recipes import Kuleshov2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.ensemble._forest.RandomForestClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  15\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.5\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): ThoughtVector(\n",
      "        (word_embedding):  WordEmbedding\n",
      "        (metric):  max_euclidean\n",
      "        (threshold):  -0.2\n",
      "        (window_size):  inf\n",
      "        (skip_text_shorter_than_window):  False\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): GPT2(\n",
      "        (max_log_prob_diff):  2.0\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|██▉                          | 1/10 [00:15<02:16, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (74%)]] --> [[Positive (62%)]]\n",
      "\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold [[schwarzenegger]] , jean-claud van damme or steven segal .\n",
      "\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold [[arnold]] , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|█████▊                       | 2/10 [00:32<02:10, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (72%)]] --> [[Positive (53%)]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[vision]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded [[conceptions]] of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:  30%|████████▋                    | 3/10 [00:32<01:16, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (79%)]]\n",
      "\n",
      "[[effective]] but too-tepid biopic\n",
      "\n",
      "[[potency]] but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 0 / 1 / 5:  50%|██████████████▌              | 5/10 [00:39<00:39,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (75%)]] --> [[Negative (58%)]]\n",
      "\n",
      "if you sometimes like to go to the movies to have [[fun]] , wasabi is a good place to start .\n",
      "\n",
      "if you sometimes like to go to the movies to have [[amusement]] , wasabi is a good place to start .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (64%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 0 / 1 / 6:  60%|█████████████████▍           | 6/10 [00:49<00:32,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (90%)]] --> [[Positive (65%)]]\n",
      "\n",
      "the [[film]] provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "the [[movie]] provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 0 / 1 / 7:  70%|████████████████████▎        | 7/10 [00:52<00:22,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[Negative (72%)]]\n",
      "\n",
      "offers that [[rare]] combination of entertainment and education .\n",
      "\n",
      "offers that [[meager]] combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 0 / 2 / 9:  90%|██████████████████████████   | 9/10 [01:01<00:06,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (70%)]] --> [[Negative (74%)]]\n",
      "\n",
      "perhaps no picture [[ever]] made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "perhaps no picture [[already]] made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (59%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 0 / 2 / 10: 100%|███████████████████████████| 10/10 [01:07<00:00,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (87%)]] --> [[Positive (58%)]]\n",
      "\n",
      "take care of my cat offers a refreshingly different slice of asian [[cinema]] .\n",
      "\n",
      "take care of my cat offers a refreshingly different slice of asian [[movie]] .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 8      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 2      |\n",
      "| Original accuracy:            | 80.0%  |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 8.27%  |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 113.88 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(rf_model_wrapper,Kuleshov2017,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: TF-IDF and Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  15\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.5\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): ThoughtVector(\n",
      "        (word_embedding):  WordEmbedding\n",
      "        (metric):  max_euclidean\n",
      "        (threshold):  -0.2\n",
      "        (window_size):  inf\n",
      "        (skip_text_shorter_than_window):  False\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): GPT2(\n",
      "        (max_log_prob_diff):  2.0\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|██▉                          | 1/10 [00:14<02:06, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Positive (59%)]]\n",
      "\n",
      "the rock is [[destined]] to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "the rock is [[meant]] to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|█████▊                       | 2/10 [00:30<02:00, 15.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[Positive (59%)]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[lord]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[god]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 1 / 4:  40%|███████████▌                 | 4/10 [00:30<00:45,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[Negative (63%)]]\n",
      "\n",
      "[[effective]] but too-tepid biopic\n",
      "\n",
      "[[potency]] but too-tepid biopic\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (68%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 0 / 1 / 5:  50%|██████████████▌              | 5/10 [00:38<00:38,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[Negative (50%)]]\n",
      "\n",
      "emerges as something rare , an issue movie that's so honest and keenly [[observed]] that it doesn't feel like one .\n",
      "\n",
      "emerges as something rare , an issue movie that's so honest and keenly [[followed]] that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 0 / 1 / 6:  60%|█████████████████▍           | 6/10 [00:47<00:31,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (57%)]]\n",
      "\n",
      "the film [[provides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "the film [[offered]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 0 / 1 / 7:  70%|████████████████████▎        | 7/10 [00:50<00:21,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (95%)]] --> [[Positive (63%)]]\n",
      "\n",
      "offers that [[rare]] combination of entertainment and education .\n",
      "\n",
      "offers that [[scant]] combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 0 / 2 / 9:  90%|██████████████████████████   | 9/10 [00:58<00:06,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[Positive (55%)]]\n",
      "\n",
      "perhaps no picture [[ever]] made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "perhaps no picture [[already]] made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 0 / 2 / 10: 100%|███████████████████████████| 10/10 [01:09<00:00,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (99%)]] --> [[Positive (58%)]]\n",
      "\n",
      "take care of my cat offers a refreshingly different [[slice]] of asian [[cinema]] .\n",
      "\n",
      "take care of my cat offers a refreshingly different [[portion]] of asian [[theater]] .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 8      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 2      |\n",
      "| Original accuracy:            | 80.0%  |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 9.23%  |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 128.88 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(lg_model_wrapper,Kuleshov2017,text_attack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  15\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.5\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): ThoughtVector(\n",
      "        (word_embedding):  WordEmbedding\n",
      "        (metric):  max_euclidean\n",
      "        (threshold):  -0.2\n",
      "        (window_size):  inf\n",
      "        (skip_text_shorter_than_window):  False\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): GPT2(\n",
      "        (max_log_prob_diff):  2.0\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|██▉                          | 1/10 [00:14<02:07, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (84%)]] --> [[Positive (60%)]]\n",
      "\n",
      "the rock is [[destined]] to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "the rock is [[meant]] to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|█████▊                       | 2/10 [00:30<02:02, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (82%)]] --> [[Negative (51%)]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[lord]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the [[god]] of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 1 / 4:  40%|███████████▌                 | 4/10 [00:30<00:46,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[Negative (54%)]]\n",
      "\n",
      "[[effective]] but too-tepid biopic\n",
      "\n",
      "[[potency]] but too-tepid biopic\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (57%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 0 / 1 / 5:  50%|██████████████▌              | 5/10 [00:38<00:38,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (55%)]]\n",
      "\n",
      "emerges as something rare , an issue movie that's so [[honest]] and keenly observed that it doesn't feel like one .\n",
      "\n",
      "emerges as something rare , an issue movie that's so [[frank]] and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 0 / 1 / 6:  60%|█████████████████▍           | 6/10 [00:48<00:32,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[Negative (59%)]]\n",
      "\n",
      "the film [[provides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "the film [[provide]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 0 / 1 / 7:  70%|████████████████████▎        | 7/10 [00:51<00:21,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[Positive (62%)]]\n",
      "\n",
      "offers that [[rare]] combination of entertainment and education .\n",
      "\n",
      "offers that [[scant]] combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 0 / 2 / 9:  90%|██████████████████████████   | 9/10 [00:58<00:06,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[Positive (52%)]]\n",
      "\n",
      "perhaps no picture [[ever]] made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "perhaps no picture [[already]] made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 0 / 2 / 10: 100%|███████████████████████████| 10/10 [01:09<00:00,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (98%)]] --> [[Positive (54%)]]\n",
      "\n",
      "take care of my cat offers a [[refreshingly]] different slice of asian [[cinema]] .\n",
      "\n",
      "take care of my cat offers a [[elegantly]] different slice of asian [[theater]] .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 8      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 2      |\n",
      "| Original accuracy:            | 80.0%  |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 9.23%  |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 128.88 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(bow_lg_model_wrapper,Kuleshov2017,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruthi2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an algorithim that swaps characters either by inserting or deleting them. It targets characters that are on adjacent keys on a QWERTY keyboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.attack_recipes import Pruthi2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.ensemble._forest.RandomForestClassifier'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapNeighboringCharacterSwap(\n",
      "        (random_one):  False\n",
      "      )\n",
      "    (1): WordSwapRandomCharacterDeletion(\n",
      "        (random_one):  False\n",
      "      )\n",
      "    (2): WordSwapRandomCharacterInsertion(\n",
      "        (random_one):  False\n",
      "      )\n",
      "    (3): WordSwapQWERTY\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_num_words):  1\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): MinWordLength\n",
      "    (2): StopwordModification\n",
      "    (3): RepeatModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|██▉                          | 1/10 [00:03<00:27,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (74%)]] --> [[[FAILED]]]\n",
      "\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|█████▊                       | 2/10 [00:08<00:32,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (72%)]] --> [[[FAILED]]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 2 / 0 / 3:  30%|████████▋                    | 3/10 [00:08<00:20,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (78%)]]\n",
      "\n",
      "[[effective]] but too-tepid biopic\n",
      "\n",
      "[[eSffective]] but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 3 / 0 / 4:  40%|███████████▌                 | 4/10 [00:09<00:14,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (75%)]] --> [[[FAILED]]]\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 3 / 0 / 5:  50%|██████████████▌              | 5/10 [00:11<00:11,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (64%)]] --> [[Negative (54%)]]\n",
      "\n",
      "emerges as something [[rare]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "emerges as something [[rEare]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 4 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:13<00:09,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (90%)]] --> [[[FAILED]]]\n",
      "\n",
      "the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 4 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:14<00:06,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[Negative (71%)]]\n",
      "\n",
      "offers that [[rare]] combination of entertainment and education .\n",
      "\n",
      "offers that [[rae]] combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 5 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:16<00:01,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (70%)]] --> [[[FAILED]]]\n",
      "\n",
      "perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (59%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 6 / 1 / 10: 100%|███████████████████████████| 10/10 [00:18<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (87%)]] --> [[[FAILED]]]\n",
      "\n",
      "take care of my cat offers a refreshingly different slice of asian cinema .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 3      |\n",
      "| Number of failed attacks:     | 6      |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 60.0%  |\n",
      "| Attack success rate:          | 33.33% |\n",
      "| Average perturbed word %:     | 14.25% |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 353.89 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(rf_model_wrapper,Pruthi2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words: TF-IDF and Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapNeighboringCharacterSwap(\n",
      "        (random_one):  False\n",
      "      )\n",
      "    (1): WordSwapRandomCharacterDeletion(\n",
      "        (random_one):  False\n",
      "      )\n",
      "    (2): WordSwapRandomCharacterInsertion(\n",
      "        (random_one):  False\n",
      "      )\n",
      "    (3): WordSwapQWERTY\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_num_words):  1\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): MinWordLength\n",
      "    (2): StopwordModification\n",
      "    (3): RepeatModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|██▉                          | 1/10 [00:02<00:24,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[[FAILED]]]\n",
      "\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|█████▊                       | 2/10 [00:06<00:27,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[[FAILED]]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 3 / 0 / 3:  30%|████████▋                    | 3/10 [00:07<00:17,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[[FAILED]]]\n",
      "\n",
      "effective but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 3 / 0 / 4:  40%|███████████▌                 | 4/10 [00:08<00:12,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (68%)]] --> [[Negative (81%)]]\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a [[good]] place to start .\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a [[god]] place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 4 / 0 / 5:  50%|██████████████▌              | 5/10 [00:10<00:10,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (83%)]] --> [[[FAILED]]]\n",
      "\n",
      "emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 4 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:11<00:07,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (55%)]]\n",
      "\n",
      "the film [[provides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "the film [[pMrovides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 5 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:13<00:05,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (95%)]] --> [[[FAILED]]]\n",
      "\n",
      "offers that rare combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 6 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:14<00:01,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[[FAILED]]]\n",
      "\n",
      "perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 7 / 1 / 10: 100%|███████████████████████████| 10/10 [00:16<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (99%)]] --> [[[FAILED]]]\n",
      "\n",
      "take care of my cat offers a refreshingly different slice of asian cinema .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 2      |\n",
      "| Number of failed attacks:     | 7      |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 70.0%  |\n",
      "| Attack success rate:          | 22.22% |\n",
      "| Average perturbed word %:     | 4.71%  |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 353.89 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(lg_model_wrapper,Pruthi2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedySearch\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapNeighboringCharacterSwap(\n",
      "        (random_one):  False\n",
      "      )\n",
      "    (1): WordSwapRandomCharacterDeletion(\n",
      "        (random_one):  False\n",
      "      )\n",
      "    (2): WordSwapRandomCharacterInsertion(\n",
      "        (random_one):  False\n",
      "      )\n",
      "    (3): WordSwapQWERTY\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_num_words):  1\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): MinWordLength\n",
      "    (2): StopwordModification\n",
      "    (3): RepeatModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|██▉                          | 1/10 [00:02<00:24,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[Positive (84%)]] --> [[[FAILED]]]\n",
      "\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|█████▊                       | 2/10 [00:07<00:28,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[Positive (82%)]] --> [[[FAILED]]]\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 3 / 0 / 3:  30%|████████▋                    | 3/10 [00:07<00:18,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[[FAILED]]]\n",
      "\n",
      "effective but too-tepid biopic\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 3 / 0 / 4:  40%|███████████▌                 | 4/10 [00:08<00:12,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[Positive (57%)]] --> [[Negative (83%)]]\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a [[good]] place to start .\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a [[god]] place to start .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 3 / 0 / 5:  50%|██████████████▌              | 5/10 [00:10<00:10,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[Positive (78%)]] --> [[Negative (56%)]]\n",
      "\n",
      "emerges as something [[rare]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "emerges as something [[rate]] , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 3 / 0 / 6:  60%|█████████████████▍           | 6/10 [00:11<00:07,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[Positive (81%)]] --> [[Negative (55%)]]\n",
      "\n",
      "the film [[provides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "the film [[pdovides]] some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 4 / 0 / 7:  70%|████████████████████▎        | 7/10 [00:13<00:05,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[[Positive (88%)]] --> [[[FAILED]]]\n",
      "\n",
      "offers that rare combination of entertainment and education .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 5 / 1 / 9:  90%|██████████████████████████   | 9/10 [00:14<00:01,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[[Positive (86%)]] --> [[[FAILED]]]\n",
      "\n",
      "perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n",
      "\n",
      "\n",
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[[Negative (54%)]] --> [[[SKIPPED]]]\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 6 / 1 / 10: 100%|███████████████████████████| 10/10 [00:16<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[[Positive (98%)]] --> [[[FAILED]]]\n",
      "\n",
      "take care of my cat offers a refreshingly different slice of asian cinema .\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 3      |\n",
      "| Number of failed attacks:     | 6      |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 60.0%  |\n",
      "| Attack success rate:          | 33.33% |\n",
      "| Average perturbed word %:     | 4.9%   |\n",
      "| Average num. words per input: | 19.5   |\n",
      "| Avg num queries:              | 353.89 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "do_attack(bow_lg_model_wrapper,Pruthi2019,text_attack_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not appear to be particularly effective as opposed to the other algorithims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these attacks are aimed at neural networks specifically. However, there are some algorithims that can target SKlearn models. In order of most effective to least effective these models are the following:\n",
    "1. IGA (iga_wang_2019)\n",
    "2. TextFoolerJin2019\n",
    "3. Kuleshov2017\n",
    "4. Pruthi2019\n",
    "5. Checklist\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
